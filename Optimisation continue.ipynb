{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation sans contraintes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode du gradient à pas constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (gradient_rho_constant): 0.03553509712219238\n",
      "[-0.68823504  0.15332241 -0.61439915  0.48960858 -0.04326259]\n"
     ]
    }
   ],
   "source": [
    "A,B,S = definition_constantes()\n",
    "d=B.shape[0]\n",
    "x0 = np.ones((d,))\n",
    "\n",
    "debut = time.time()\n",
    "GradResults=gradient_rho_constant(f1,df1,x0,rho=0.01,tol=1e-6,args=(B,S))\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (gradient_rho_constant):',tps_ecoule)\n",
    "print(GradResults['minimum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradResults['converged']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaie des valeurs de $\\rho$ dans la grille [1, 0.1, 0.01, 0.001, 0.0001] pour vérifier les convergences, et on comparera le minimum de f1 pour les valeurs de $\\rho$ résultant en une convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehdiboubnan/Documents/3A/Git Project Teams/Optimization-Team/Optimization/Codes/main.py:57: RuntimeWarning: invalid value encountered in subtract\n",
      "  xnp1=xn-rho*dfx             # nouveau point courant (x_{n+1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No convergence for rho =  1\n",
      "No convergence for rho =  0.1\n",
      "No convergence for rho =  0.0001\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rho_grid = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "rho_convergence = []\n",
    "f_minimum_results = []\n",
    "rho_no_convergence = []\n",
    "for rho in rho_grid:\n",
    "    GradResults = gradient_rho_constant(f1,df1,x0,rho=rho,tol=1e-6,args=(B,S))\n",
    "    if GradResults['converged']:\n",
    "        rho_convergence.append(rho)\n",
    "        f_minimum_results.append(GradResults['f_minimum'])\n",
    "        plt.plot(range(GradResults[\"iterations\"] + 1), GradResults[\"f_values\"], label = str(rho))\n",
    "        \n",
    "    else:\n",
    "        print('No convergence for rho = ', rho)\n",
    "        rho_no_convergence.append(rho)\n",
    "        \n",
    "plt.legend() \n",
    "# plt.ylim((-10, 120))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a des convergences stables pour $\\rho \\in \\{0.01, 0.001 \\}$, $\\rho = 0.01$ donne une convergence plus rapide, ce qui est normal vu que c'est un pas d'apprentissage plus grand que $0.001$ et qui n'entraîne pas de divergence de la fonction autour du minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode du gradient à pas adptatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le choix d'un pas constant peut s'avérer contraignant, en effet il faut choisir un pas qui résulte en une bonne exploration de l'espace de notre variable sans pour autant diverger du minimum. On pourra par exemple augmenter le pas lorsque $f(x_{n+1}) \\le f(x_n)$ et le diminuer sinon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_rho_adaptatif(fun, fun_der, U0, rho, tol,args):\n",
    "\n",
    "    itermax=10000  # nombre maximal d'itérations \n",
    "    xn=U0\n",
    "    f=fun(xn,*args) # point initial de l'algorithme\n",
    "    it=0         # compteur pour les itérations\n",
    "    converged = False\n",
    "    x_values = [xn]\n",
    "    f_values = [f]\n",
    "    \n",
    "    while (~converged & (it < itermax)):\n",
    "        it=it+1\n",
    "        dfx=fun_der(xn,*args)       # valeur courante de la fonction à minimiser\n",
    "        xnp1=xn-rho*dfx             # nouveau point courant (x_{n+1})\n",
    "        fnp1=fun(xnp1,*args)\n",
    "        if fnp1 < fun(xn,*args):\n",
    "            rho = rho*2\n",
    "            \n",
    "        else:\n",
    "            rho = rho/2\n",
    "            \n",
    "        if abs(fnp1 - f) < tol:\n",
    "            converged = True\n",
    "        xn=xnp1; f=fnp1;           # xnp1 : nouveau point courant\n",
    "        x_values.append(xn)\n",
    "        f_values.append(f)\n",
    "\n",
    "    GradResults = {\n",
    "            'initial_x':U0,\n",
    "            'minimum':xnp1,\n",
    "            'f_minimum':fnp1,\n",
    "            'iterations':it,\n",
    "            'converged':converged,\n",
    "            'x_values':x_values,\n",
    "            'f_values':f_values\n",
    "            }\n",
    "    return GradResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (gradient_rho_constant): 0.023088932037353516\n",
      "[-0.6813617   0.14924945 -0.61429956  0.48568951 -0.0336978 ]\n"
     ]
    }
   ],
   "source": [
    "A,B,S = definition_constantes()\n",
    "d=B.shape[0]\n",
    "x0 = np.ones((d,))\n",
    "\n",
    "debut = time.time()\n",
    "GradResults=gradient_rho_adaptatif(f1,df1,x0,rho=0.001,tol=1e-6,args=(B,S))\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (gradient_rho_constant):',tps_ecoule)\n",
    "print(GradResults['minimum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste notre algorithme adaptatif sur la même grille que précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No convergence for rho =  0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rho_grid = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "rho_convergence = []\n",
    "f_minimum_results = []\n",
    "rho_no_convergence = []\n",
    "for rho in rho_grid:\n",
    "    GradResults=gradient_rho_adaptatif(f1,df1,x0,rho=rho,tol=1e-6,args=(B,S))\n",
    "    if GradResults['converged']:\n",
    "        rho_convergence.append(rho)\n",
    "        f_minimum_results.append(GradResults['f_minimum'])\n",
    "        plt.plot(range(GradResults[\"iterations\"] + 1), GradResults[\"f_values\"], label = str(rho))\n",
    "        \n",
    "    else:\n",
    "        print('No convergence for rho = ', rho)\n",
    "        rho_no_convergence.append(rho)\n",
    "        \n",
    "plt.legend() \n",
    "plt.ylim((-10, 120))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que la \"qualité\" de la convergence dépend du $\\rho$, elle est lente et instable pour $\\rho = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit une valeur de \"bonne\" valeur $\\rho$ pour chaque méthode et on compare les performances.\n",
    "On prend alors $\\rho = 0.01$ pour gradient_rho_constant et $\\rho = 0.1$ pour gradient_rho_adaptatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (gradient_rho_constant): 0.053941965103149414\n",
      "[-0.68577502  0.1518605  -0.61419559  0.48823926 -0.03959053]\n",
      "Nombre d'itérations :  641\n"
     ]
    }
   ],
   "source": [
    "rho_const = 0.01\n",
    "A,B,S = definition_constantes()\n",
    "d=B.shape[0]\n",
    "x0 = np.ones((d,))\n",
    "\n",
    "debut = time.time()\n",
    "GradResultsConstant=gradient_rho_constant(f1,df1,x0,rho=rho_const,tol=1e-6,args=(B,S))\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (gradient_rho_constant):',tps_ecoule)\n",
    "print(GradResults['minimum'])\n",
    "print(\"Nombre d'itérations : \", GradResultsConstant['iterations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (gradient_rho_constant): 0.018518924713134766\n",
      "[-0.68577502  0.1518605  -0.61419559  0.48823926 -0.03959053]\n",
      "Nombre d'itérations :  124\n"
     ]
    }
   ],
   "source": [
    "rho_ad = 0.1\n",
    "A,B,S = definition_constantes()\n",
    "d=B.shape[0]\n",
    "x0 = np.ones((d,))\n",
    "\n",
    "debut = time.time()\n",
    "GradResultsAdaptatif=gradient_rho_adaptatif(f1,df1,x0,rho=rho_ad,tol=1e-6,args=(B,S))\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (gradient_rho_constant):',tps_ecoule)\n",
    "print(GradResults['minimum'])\n",
    "print(\"Nombre d'itérations : \", GradResultsAdaptatif['iterations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode adaptative converge plus rapidement que la méthode à pas constant pour nos valeurs de $\\rho$ choisies, le nombre d'itérations est beaucoup plus petit: 124 face à 641, et les appels à la fonction coût par itération reste le même entre les deux méthodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de Quasi-Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (BFGS): 0.0015308856964111328\n",
      "Valeur du minimum de f1 :  -1.836962311965253\n",
      "Nombre d'itérations :  10\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "debut = time.time()\n",
    "NewtonResults = minimize(f1, x0, jac = df1, args = (B,S), method = 'BFGS', tol=1e-6)\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (BFGS):', tps_ecoule)\n",
    "print('Valeur du minimum de f1 : ', NewtonResults.fun)\n",
    "print(\"Nombre d'itérations : \", NewtonResults.nit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On converge vers le même minimum pour f1 beaucoup plus rapidement en temps d'éxécution et en nombre d'itération (10 face à 124 de la méthode du gradient à pas adaptatif et 641 de la méthode du gradient à pas constant). L'autre avantage des méthodes de quasi-newton est qu'on ne choisit pas le pas $\\rho$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation sous contraintes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Quadratic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (BFGS): 0.005149364471435547\n",
      "Valeur du minimum de f1 :  -0.13853161423986765\n",
      "Nombre d'itérations :  9\n",
      "Valeur du minimum :  [1.08020986e-15 1.26984963e-01 2.70234387e-15 1.94536212e-02\n",
      " 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "debut = time.time()\n",
    "SQPResults = minimize(f1, x0, jac = df1, args = (B,S), method = 'SLSQP', tol=1e-6, bounds = [(0, 1) for k in range(5)])\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (BFGS):', tps_ecoule)\n",
    "print('Valeur du minimum de f1 : ', SQPResults.fun)\n",
    "print(\"Nombre d'itérations : \", SQPResults.nit)\n",
    "print(\"Valeur du minimum : \", SQPResults.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a bien le minimum qui respect la contrainte $U_{ad}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode de pénalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit la pénalisation $\\beta$ sur $U_{ad} = [0, 1]^5$ de la manière suivante:\n",
    "\\begin{equation}\n",
    "\\beta(x) = \\sum_{i = 1}^{5}[(-x_i)^{+} + (x_i - 1)^{+}]^2\n",
    "\\end{equation}\n",
    "où x = [x_1, x_2, x_3, x_4, x_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Description\n",
    "    -----------\n",
    "    La pénalisation de x sur le pavé Uad = [0, 1]^5\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    x : list of 5 elements. The current point of the algorithm.\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    Float, la pénalisation sur le pavé.\n",
    "    \n",
    "    \"\"\"\n",
    "    s = 0\n",
    "    for xi in x:\n",
    "        s += (max(0, -xi) + max(0, xi - 1))**2\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on définit les fonctions pénalisation de f1 et f2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1penal(U, B, S, epsilon):\n",
    "    return f1(U, B, S) + beta(U)/epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(U, S):\n",
    "    n=U.shape[0]\n",
    "    U=np.matrix(U)\n",
    "    U.shape=(n,1)\n",
    "    fU = np.transpose(U) * S * U + np.transpose(U) * np.exp(U);\n",
    "    return float(fU)\n",
    "    \n",
    "def f2penal(U, S, epsilon):\n",
    "    return f2(U, S) + beta(U)/epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans notre algorithme d'optimisation avec pénalisation, on considère la suite décroissante $(\\epsilon_n)$ où $\\epsilon_n = \\frac{1}{2^n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour que la fonction optimize_penalty_bis agisse sur les deux fonctions f1 et f2 (en réalité elle prend comme argument la fonction pénalisé). Il faut définir f1penal (respectivement f2penal) qui prend epsilon comme dernier argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_penalty_bis(fun, U0, tol, args):\n",
    "    \n",
    "    itermax = 10000  # nombre maximal d'itérations \n",
    "    xn = U0\n",
    "    f = fun(xn, *args) # point initial de l'algorithme\n",
    "    it = 0         # compteur pour les itérations\n",
    "    converged = False;\n",
    "    \n",
    "    while (~converged & (it < itermax)):\n",
    "        it=it+1\n",
    "        xnp1 = minimize(fun, xn, args = args, method = 'BFGS', tol = 1e-6).x\n",
    "        fnp1 = fun(xnp1, *args)\n",
    "        if abs(fnp1-f) < tol:\n",
    "            converged = True\n",
    "        xn=xnp1; f=fnp1;            # xnp1 : nouveau point courant\n",
    "        args = list(args)\n",
    "        args[-1] /= 2\n",
    "        args = tuple(args)\n",
    "\n",
    "    PenaltyResults = {\n",
    "            'initial_x':U0,\n",
    "            'minimum':xnp1,\n",
    "            'f_minimum':fnp1,\n",
    "            'iterations':it,\n",
    "            'converged':converged\n",
    "            }\n",
    "    return PenaltyResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation avec pénalisation de f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (BFGS): 0.34787487983703613\n",
      "Valeur du minimum de f1 :  -0.13853243975221347\n",
      "Nombre d'itérations :  24\n",
      "Valeur du minimum :  [-3.53161469e-08  1.26885928e-01 -2.47327616e-07  1.94103685e-02\n",
      " -2.06874547e-08]\n"
     ]
    }
   ],
   "source": [
    "debut = time.time()\n",
    "epsilon0 = 1\n",
    "args = (B, S, epsilon0)\n",
    "PenaltyResults1 = optimize_penalty_bis(f1penal, x0, 1e-6, args)\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (BFGS):', tps_ecoule)\n",
    "print('Valeur du minimum de f1 : ', PenaltyResults1['f_minimum'])\n",
    "print(\"Nombre d'itérations : \", PenaltyResults1['iterations'])\n",
    "print(\"Valeur du minimum : \", PenaltyResults1['minimum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation avec pénalisation de f2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (BFGS): 0.2063443660736084\n",
      "Valeur du minimum de f2 :  -5.95457479883209e-07\n",
      "Nombre d'itérations :  22\n",
      "Valeur du minimum :  [-2.45870177e-07 -2.45866973e-07 -2.45864400e-07 -2.45865000e-07\n",
      " -2.45865302e-07]\n"
     ]
    }
   ],
   "source": [
    "debut = time.time()\n",
    "epsilon0 = 1\n",
    "args = (S, epsilon0)\n",
    "PenaltyResults2 = optimize_penalty_bis(f2penal, x0, 1e-6, args)\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (BFGS):', tps_ecoule)\n",
    "print('Valeur du minimum de f2 : ', PenaltyResults2['f_minimum'])\n",
    "print(\"Nombre d'itérations : \", PenaltyResults2['iterations'])\n",
    "print(\"Valeur du minimum : \", PenaltyResults2['minimum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérification des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions les résultats sur f1 et f2 obtenus avec la méthode par pénalisation en les comparant avec les résultats obtenus avec un algorithme SQP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (SLSQP): 0.0041730403900146484\n",
      "Valeur du minimum de f1 :  -0.13853161426486305\n",
      "Nombre d'itérations :  9\n",
      "Valeur du minimum :  [0.00000000e+00 1.26984956e-01 0.00000000e+00 1.94536138e-02\n",
      " 2.16628331e-15]\n"
     ]
    }
   ],
   "source": [
    "debut = time.time()\n",
    "SQPResults1 = minimize(f1, x0, args = (B, S), method = 'SLSQP', tol=1e-6, bounds = [(0, 1) for k in range(5)])\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (SLSQP):', tps_ecoule)\n",
    "print('Valeur du minimum de f1 : ', SQPResults1.fun)\n",
    "print(\"Nombre d'itérations : \", SQPResults1.nit)\n",
    "print(\"Valeur du minimum : \", SQPResults1.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (SLSQP): 0.0036458969116210938\n",
      "Valeur du minimum de f2 :  2.976067656614975e-15\n",
      "Nombre d'itérations :  4\n",
      "Valeur du minimum :  [1.72929872e-16 2.17964615e-15 5.95736056e-16 0.00000000e+00\n",
      " 2.77555756e-17]\n"
     ]
    }
   ],
   "source": [
    "debut = time.time()\n",
    "SQPResults2 = minimize(f2, x0, args = (S), method = 'SLSQP', tol=1e-6, bounds = [(0, 1) for k in range(5)])\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (SLSQP):', tps_ecoule)\n",
    "print('Valeur du minimum de f2 : ', SQPResults2.fun)\n",
    "print(\"Nombre d'itérations : \", SQPResults2.nit)\n",
    "print(\"Valeur du minimum : \", SQPResults2.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons les différences entre les résultats des deux approches. On prend la norm de la différence entre les points minimum (chacun obtenu par une approche) et les valeurs de f1 et f2 en points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Différence des minimums pour f1 :  0.00010805959170449629\n",
      "Différence des minimums pour f2 :  5.497739193420062e-07\n",
      "Différence des valeurs de f1 en les minimums :  8.254873504198734e-07\n",
      "Différence des valeurs de f2 en les minimums :  5.954574828592766e-07\n"
     ]
    }
   ],
   "source": [
    "print('Différence des minimums pour f1 : ', np.linalg.norm(SQPResults1.x - PenaltyResults1['minimum']))\n",
    "print('Différence des minimums pour f2 : ', np.linalg.norm(SQPResults2.x - PenaltyResults2['minimum']))\n",
    "\n",
    "print('Différence des valeurs de f1 en les minimums : ', abs(SQPResults1.fun - PenaltyResults1['f_minimum']))\n",
    "print('Différence des valeurs de f2 en les minimums : ', abs(SQPResults2.fun - PenaltyResults2['f_minimum']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient donc bien des résultats très proches moyennant ces deux approches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Méthodes duales pour l'optimisation sous contraintes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1(x, B, S, lambd):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Lagrangien associé à f1 et à la contrainte Uad = [0, 1]^5\n",
    "    \n",
    "    \"\"\"\n",
    "    s = f1(x, B, S)\n",
    "    for i in range(10):\n",
    "        if i <= 4:\n",
    "            s += lambd[i]*(-x[i])\n",
    "            \n",
    "        else:\n",
    "            s += lambd[i]*(x[i%5] - 1)\n",
    "            \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Uzawa(fun, U0, rho, tol, args):\n",
    "    \n",
    "    itermax = 10000  # nombre maximal d'itérations \n",
    "    xn = U0\n",
    "    lambdn = args[-1] # lambda\n",
    "    f = fun(xn, *args) # point initial de l'algorithme\n",
    "    it = 0         # compteur pour les itérations\n",
    "    converged = False;\n",
    "    \n",
    "    while (~converged & (it < itermax)):\n",
    "        it = it + 1\n",
    "        xnp1 = minimize(fun, xn, args = args, method = 'BFGS', tol = 1e-6).x\n",
    "        fnp1 = fun(xnp1, *args)\n",
    "        if abs(fnp1-f) < tol:\n",
    "            converged = True\n",
    "        xn=xnp1; f=fnp1;            # xnp1 : nouveau point courant\n",
    "        args = list(args)\n",
    "        gradlambd = np.array([-xn[i] for i in range(5)] + [(xn[i] - 1) for i in range(5)])\n",
    "#         args[-1] += np.array([max(rho*gradlambd[i], 0) for i in range(len(gradlambd))]) # Update lambda\n",
    "        args[-1] += rho*gradlambd\n",
    "        args[-1] = np.array([max(args[-1][i], 0) for i in range(len(gradlambd))])\n",
    "        args = tuple(args)\n",
    "    \n",
    "    UzawaResults = {\n",
    "            'initial_x':U0,\n",
    "            'minimum':xnp1,\n",
    "            'f_minimum':fnp1,\n",
    "            'iterations':it,\n",
    "            'converged':converged\n",
    "            }\n",
    "    return UzawaResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application sur f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (Uzawa): 8.579363107681274\n",
      "Valeur du minimum de f1 :  -0.13853176769095196\n",
      "Nombre d'itérations :  3797\n",
      "Valeur du minimum :  [ 1.78473145e-05  1.26907620e-01 -2.16153175e-05  1.94260530e-02\n",
      " -1.45058540e-05]\n"
     ]
    }
   ],
   "source": [
    "debut = time.time()\n",
    "lambd0 = np.ones(10)\n",
    "args = (B, S, lambd0)\n",
    "UzawaResults1 = Uzawa(L1, x0, 0.1, 1e-10, args)\n",
    "tps_ecoule = time.time() - debut\n",
    "print('tps écoulé (Uzawa):', tps_ecoule)\n",
    "print('Valeur du minimum de f1 : ', UzawaResults1['f_minimum'])\n",
    "print(\"Nombre d'itérations : \", UzawaResults1['iterations'])\n",
    "print(\"Valeur du minimum : \", UzawaResults1['minimum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.13863072387264566"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(UzawaResults1['minimum'], B, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On compare les minimums de f1 obtenus avec les méthodes Uzawa et SQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Différence des minimums pour f1 :  8.795838959798934e-05\n"
     ]
    }
   ],
   "source": [
    "print('Différence des minimums pour f1 : ', np.linalg.norm(SQPResults1.x - UzawaResults1['minimum']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux méthodes convergent bien vers le même minimum unique de f1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application sur f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(x, S, lambd):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Lagrangien associé à f1 et à la contrainte Uad = [0, 1]^5\n",
    "    \n",
    "    \"\"\"\n",
    "    s = f2(x, S)\n",
    "    for i in range(10):\n",
    "        if i <= 4:\n",
    "            s += lambd[i]*(-x[i])\n",
    "            \n",
    "        else:\n",
    "            s += lambd[i]*(x[i%5] - 1)\n",
    "            \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (Uzawa): 6.900090932846069\n",
      "Valeur du minimum de f2 :  3.069075884807169e-15\n",
      "Nombre d'itérations :  10000\n",
      "Valeur du minimum :  [-2.10674299e-09  1.14633498e-09  4.08011906e-09  2.01751742e-09\n",
      "  3.15761027e-09]\n"
     ]
    }
   ],
   "source": [
    "debut = time.time()\n",
    "lambd0 = np.ones(10)\n",
    "args = (S, lambd0)\n",
    "UzawaResults2 = Uzawa(L2, x0, 0.1, 1e-20, args)\n",
    "tps_ecoule = time.time() - debut\n",
    "print('tps écoulé (Uzawa):', tps_ecoule)\n",
    "print('Valeur du minimum de f2 : ', UzawaResults2['f_minimum'])\n",
    "print(\"Nombre d'itérations : \", UzawaResults2['iterations'])\n",
    "print(\"Valeur du minimum : \", UzawaResults2['minimum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérification des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour la méthode de pénalisation, comparons maintenant les résultats obtenus par l'algorithme d'Uzawa et par un algorithme SQP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Différence des minimums pour f1 :  8.795838959798934e-05\n",
      "Différence des minimums pour f2 :  6.036612567708126e-09\n",
      "Différence des valeurs de f1 en les minimums :  1.5342608891688592e-07\n",
      "Différence des valeurs de f2 en les minimums :  9.300822819219412e-17\n"
     ]
    }
   ],
   "source": [
    "print('Différence des minimums pour f1 : ', np.linalg.norm(SQPResults1.x - UzawaResults1['minimum']))\n",
    "print('Différence des minimums pour f2 : ', np.linalg.norm(SQPResults2.x - UzawaResults2['minimum']))\n",
    "\n",
    "print('Différence des valeurs de f1 en les minimums : ', abs(SQPResults1.fun - UzawaResults1['f_minimum']))\n",
    "print('Différence des valeurs de f2 en les minimums : ', abs(SQPResults2.fun - UzawaResults2['f_minimum']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient donc bien des résultats très proches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation non convexe - Recuit simulé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(U, B, S):\n",
    "    \n",
    "    return f1(U, B, S) + 10*np.sin(2*f1(U, B, S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimisation par BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On essaie les points initiaux [1, 1, 1, 1, 1] et [0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (BFGS): 0.03492617607116699\n",
      "Valeur du minimum de f3 :  17.47643311291821\n",
      "Valeur du point minimisant de f3 :  [1.13773313 0.69527366 0.31701339 0.53778966 0.50675153]\n",
      "Nombre d'itérations :  6\n"
     ]
    }
   ],
   "source": [
    "debut = time.time()\n",
    "x0 = np.ones((d,))\n",
    "NewtonResults = minimize(f3, x0, args = (B, S), method = 'BFGS', tol=1e-10)\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (BFGS):', tps_ecoule)\n",
    "print('Valeur du minimum de f3 : ', NewtonResults.fun)\n",
    "print('Valeur du point minimisant de f3 : ', NewtonResults.x)\n",
    "print(\"Nombre d'itérations : \", NewtonResults.nit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (BFGS): 0.03083634376525879\n",
      "Valeur du minimum de f3 :  -10.797900769519343\n",
      "Valeur du point minimisant de f3 :  [-0.08806077  0.17642997 -0.337911    0.05741153  0.12232578]\n",
      "Nombre d'itérations :  8\n"
     ]
    }
   ],
   "source": [
    "debut = time.time()\n",
    "x0 = np.zeros((d,))\n",
    "NewtonResults = minimize(f3, x0, args = (B, S), method = 'BFGS', tol=1e-10)\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (BFGS):', tps_ecoule)\n",
    "print('Valeur du minimum de f3 : ', NewtonResults.fun)\n",
    "print('Valeur du point minimisant de f3 : ', NewtonResults.x)\n",
    "print(\"Nombre d'itérations : \", NewtonResults.nit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ne converge pas vers les mêmes minimas locaux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste l'algorithme Simulated Annealing avec une liste de points initiaux pour voir si on converge toujours vers le même minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tps écoulé (SimulatedAnnealing): 20.395237922668457\n",
      "Valeur du minimum de f3 :  -10.797900769519423\n",
      "Valeur du point minimisant de f3 :  [ 0.09312001 -0.06746167 -0.29671142 -0.03672598  0.60098799]\n",
      "Nombre d'itérations :  1000\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import basinhopping\n",
    "debut = time.time()\n",
    "x0 = np.ones(d)\n",
    "args = (B, S)\n",
    "SimulatedAnnealingResults = basinhopping(f3, x0, 1000, minimizer_kwargs = {\"args\" : (B, S)})\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (SimulatedAnnealing):', tps_ecoule)\n",
    "print('Valeur du minimum de f3 : ', SimulatedAnnealingResults.fun)\n",
    "print('Valeur du point minimisant de f3 : ', SimulatedAnnealingResults.x)\n",
    "print(\"Nombre d'itérations : \", SimulatedAnnealingResults.nit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialPoints = [np.ones(d), np.zeros(d)] + [np.random.rand(d) for k in range(5)]\n",
    "f3_minimas = []\n",
    "f3_values = []\n",
    "for x0 in initialPoints:\n",
    "    SimulatedAnnealingResults = basinhopping(f3, x0, 1000, minimizer_kwargs = {\"args\" : (B, S)})\n",
    "    f3_minimas.append(SimulatedAnnealingResults.x)\n",
    "    f3_values.append(SimulatedAnnealingResults.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On converge bien vers le même minimum de la fonction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthèse d'un filtre à réponse impulsionnelle finie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(h, mu):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Description\n",
    "    -----------\n",
    "    Réponse fréquentielle\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    h : List, 30 coefficients. C'est la variable par rapport à laquelle on va minimiser H.\n",
    "    mu : Int, la variable fréquence.\n",
    "        \n",
    "    Returns\n",
    "    -----------\n",
    "    La réponse fréquentielle\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n = h.shape[0]\n",
    "    h = np.matrix(h)\n",
    "    h.shape = (n,1)\n",
    "    Cos = [np.cos(2*np.pi*mu*i) for i in range(n)] # La liste contenant les valeurs cos(2*pi*mu*i)\n",
    "    Cos = np.matrix(Cos)\n",
    "    Cos.shape = (n, 1)\n",
    "    Hh = np.transpose(h) * Cos\n",
    "    return float(Hh)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(h, p):\n",
    "    \"\"\"\n",
    "    p: Int, 3*p sera le nombre de points pour le premier intervalle et 8*p pour le second pour qu'on ait\n",
    "       une discrétisation uniforme des deux intervalles.\n",
    "    \"\"\"\n",
    "    \n",
    "    differences = []\n",
    "    Mu1 = np.linspace(0, 0.1, 3*p)\n",
    "    Mu2 = np.linspace(0.15, 0.5, 8*p)\n",
    "    for mu in Mu1:\n",
    "        differences.append(abs(1 - H(h, mu)))\n",
    "        \n",
    "    for mu in Mu2:\n",
    "        differences.append(abs(- H(h, mu)))\n",
    "        \n",
    "    return max(differences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debut = time.time()\n",
    "h0 = np.ones(30)\n",
    "p = 5\n",
    "NewtonResults = minimize(J, h0, args = (p), method = 'BFGS', tol=1e-10)\n",
    "tps_ecoule = time.time()-debut\n",
    "print('tps écoulé (BFGS):', tps_ecoule)\n",
    "print('Valeur du minimum de J : ', NewtonResults.fun)\n",
    "print(\"Nombre d'itérations : \", NewtonResults.nit)\n",
    "print(\"Etat de convergence : \", NewtonResults.success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewtonResults.message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va reformuler le problème précédent comme un problème d'optimisation linéaire:\n",
    "\n",
    "\n",
    "$$ min\\left(z \\right) $$ \n",
    "\n",
    "\n",
    "sous les contraintes:\n",
    "$$\n",
    "\\forall j\n",
    "\\begin{cases}\n",
    "H_0\\left(\\mu_j \\right) - H\\left(\\mu_j \\right) &\\le z \\\\\n",
    "H_0\\left(\\mu_j \\right) - H\\left(\\mu_j \\right) &\\ge -z\n",
    "\\end{cases}\n",
    "$$\n",
    "qu'on peut reformuler:\n",
    "$$\n",
    "\\forall j\n",
    "\\begin{cases}\n",
    "H_0\\left(\\mu_j \\right) - H\\left(\\mu_j \\right) - z &\\le 0 \\\\\n",
    "-H_0\\left(\\mu_j \\right) + H\\left(\\mu_j \\right) - z &\\le 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
